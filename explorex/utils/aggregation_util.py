"""
this intend to use dataframe offered by pandas to aggregate the data and find valuable result
TODO: generic aggregation is need to support multiple query or ad hoc query
"""

import re
from collections import Counter

import pandas as pd
from numpy import nan, float64

from explorex.utils.dataframe_util import implicit_to_dict


def default_counter(df):
    return Counter(df.apply(lambda e: str(e)[:255])).items()


def string_counter(df, transformer=lambda e: e):
    """
    designed to support multiple transformation working on the string typed column, the default transformer is
    Identity transformer, which output the same result as the input
    :param df: the dataframe to be calculated
    :param transformer: the default lambda expression output the same as the input
    :return:
    """
    return Counter(df.map(lambda e: transformer(e))).items()


def numeric_counter(df, transformer=lambda e: e):
    """
    designed to support multiple transformation on numeric typed column, the trasnform include and may include more,
    such as mean, max, histogram kinds of statics on the data
    :param df: the dataframe to be calculated
    :param transformer: the default lambda expression output the same as the input
    :return:
    """
    return Counter(df.map(lambda e: transformer(e))).items()


def dataframe_helper(df, data_type):
    """
    agg_helper is the main route for different kinds of dataframe column types to be working on
    :param df:
    :param data_type:
    :return:
    """
    if data_type is str:
        return df.aggregate(string_counter)
    if data_type is float or data_type is float64:
        return df.aggregate(numeric_counter)

    return "type \"%s\" not supported!" % str(data_type)


@implicit_to_dict
def parallel_aggregator(idx, df):
    rs = {}
    for i in df.columns:
        rs[i] = agg_helper(df[i])
    return pd.DataFrame(data=rs, columns=rs.keys(), index=[idx])


def agg_helper(df):
    """
    the df here comes from dataframe that is generated by a group behavior working on a larger dataframe.
    after the group-by operation, the larger dataframe is grouped into small dataframe, these are the input here.
    So actually ,the agg_helper is supposed to work on each of these groups and calculate some statics from it.

    blow code shows how agg_helper can help to manipulate the data

    from datetime import datetime
    startTime = datetime.now()
    daily_behaviors = grouped.aggregate({"action":agg_helper})
    print(datetime.now() - startTime)

    startTime = datetime.now()
    daily_behaviors = grouped.aggregate(agg_helper)
    print(datetime.now() - startTime)

    :param df:
    :return:
    """
    df_notnull = df[df.notnull()]
    if df_notnull.size == 0:
        return nan
    else:
        return dataframe_helper(df_notnull, type(df_notnull.iloc[0]))


# TODO: generic group by time operation
def time_aggregator(df, group_key="1d", time_col='time'):
    """
    just as below actually, the pandas built-in function can do some kinds of the works.
    So the main purpose is to allow it to operate on time field, the replaced one offered by pandas can be
    grouped = df.groupby(df['time'].map(lambda e: str(e).split(" ")[0])) thoguh this built-in should be refined
    :param group_key:
    :param time_col:
    :param df:
    :param time:
    :return:
    """
    # TODO: this method should support dynamic time, now time only can be "1D" etc
    if group_key == "1d":
        return df.groupby(df[time_col].map(lambda e: str(e).split(" ")[0]))

    return df.groupby(df[time_col].map(lambda e: str(e).split(" ")[0]))


def field_aggregator(df, col, transformer=lambda e: e):
    """
    depend on your choice, pandas built-in group-by support by this operation
    grouped = df.groupby(df['time'].map(lambda e: str(e).split(" ")[0]))
    :param df:
    :param col:
    :param transformer:
    :return:
    """
    return df.group_by(df[col].map(lambda e: transformer(e)), axis=1)


def time_range_parser(test_str):
    regex = r"^(\d+)(m|h|d|w|M|q|Q|y)$"

    matches = re.finditer(regex, test_str)

    matched_group = []

    for matchNum, match in enumerate(matches):
        for groupNum in range(0, len(match.groups())):
            groupNum += 1
            matched_group.append(match.group(groupNum))
    return matched_group


def group_counter(groups, sort_key='id', asc=False, num=10):
    flat_groups = groups.count().reset_index()
    return flat_groups.sort_values(sort_key, ascending=asc).head(num)


if __name__ == "__main__":
    print("test aggregation")
